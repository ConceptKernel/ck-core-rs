//! Kernel base class for ConceptKernel
//!
//! Provides minimal event sourcing with RBAC enforcement, URN support,
//! and inter-kernel communication via emit().
//!
//! Reference: Node.js v1.3.14 - Kernel.js

use crate::errors::{CkpError, Result};
use crate::ontology::{OntologyReader, Ontology};
use crate::rbac::PermissionChecker;
use crate::port::PortManager;
use crate::drivers::{StorageDriver, FileSystemDriver, JobFile as DriverJobFile};
use chrono::Utc;
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::PathBuf;
use std::sync::Arc;

/// Kernel base class for ConceptKernel implementation
pub struct Kernel {
    /// Root directory for concepts
    root: PathBuf,

    /// Kernel name (e.g., "Recipes.BakeCake")
    concept: Option<String>,

    /// Enable RBAC checks
    enable_rbac: bool,

    /// Loaded ontology document
    ontology: Option<Ontology>,

    /// RBAC permission checker
    permission_checker: PermissionChecker,

    /// Storage driver for backend abstraction
    driver: Arc<dyn StorageDriver>,
}

/// Job file structure written to inbox
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JobFile {
    /// Target kernel name or URN
    pub target: String,

    /// Job payload data
    pub payload: serde_json::Value,

    /// ISO 8601 timestamp
    pub timestamp: String,

    /// Transaction ID (format: {timestamp}-{shortId})
    #[serde(rename = "txId")]
    pub tx_id: String,

    /// Source kernel name or 'external'
    pub source: String,
}

/// Job handle for processing inbox jobs
///
/// Provides methods for reading job payload and archiving after processing
pub struct Job {
    /// Path to the job file
    job_path: PathBuf,

    /// Path to archive directory
    archive_dir: PathBuf,

    /// Transaction ID
    tx_id: String,

    /// Loaded job content
    content: JobFile,
}

impl Job {
    /// Get transaction ID
    pub fn tx_id(&self) -> &str {
        &self.tx_id
    }

    /// Get job payload
    pub fn payload(&self) -> &serde_json::Value {
        &self.content.payload
    }

    /// Get source kernel name
    pub fn source(&self) -> &str {
        &self.content.source
    }

    /// Get full job content
    pub fn content(&self) -> &JobFile {
        &self.content
    }

    /// Archive this job (move to archive directory)
    ///
    /// This is an atomic operation that moves the job file from inbox to archive.
    ///
    /// # Errors
    ///
    /// Returns error if file move fails
    pub fn archive(self) -> Result<()> {
        let archive_path = self.archive_dir.join(format!("{}.job", self.tx_id));

        fs::rename(&self.job_path, &archive_path)
            .map_err(|e| CkpError::IoError(format!("Failed to archive job {}: {}", self.tx_id, e)))?;

        println!("[Job] Archived job {} to {}", self.tx_id, archive_path.display());
        Ok(())
    }
}

/// Iterator over inbox jobs
pub struct InboxIterator {
    jobs: Vec<PathBuf>,
    index: usize,
    archive_dir: PathBuf,
}

impl Iterator for InboxIterator {
    type Item = Result<Job>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.index >= self.jobs.len() {
            return None;
        }

        let job_path = self.jobs[self.index].clone();
        self.index += 1;

        // Extract tx_id from filename
        let tx_id = match job_path.file_stem().and_then(|s| s.to_str()) {
            Some(id) => id.to_string(),
            None => return Some(Err(CkpError::ParseError(format!(
                "Invalid job filename: {}",
                job_path.display()
            )))),
        };

        // Read and parse job file
        let content = match fs::read_to_string(&job_path) {
            Ok(c) => c,
            Err(e) => return Some(Err(CkpError::IoError(format!(
                "Failed to read job {}: {}",
                tx_id, e
            )))),
        };

        let job_content: JobFile = match serde_json::from_str(&content) {
            Ok(j) => j,
            Err(e) => return Some(Err(CkpError::Json(e))),
        };

        Some(Ok(Job {
            job_path,
            archive_dir: self.archive_dir.clone(),
            tx_id,
            content: job_content,
        }))
    }
}

impl Kernel {
    /// Create a new Kernel instance
    ///
    /// # Arguments
    ///
    /// * `root` - Root directory for concepts
    /// * `concept` - Optional kernel name (e.g., "Recipes.BakeCake")
    /// * `enable_rbac` - Enable RBAC checks (default: true)
    ///
    /// # Example
    ///
    /// ```
    /// use ckp_core::kernel::Kernel;
    /// use std::path::PathBuf;
    ///
    /// let kernel = Kernel::new(
    ///     PathBuf::from("/concepts"),
    ///     Some("Recipes.BakeCake".to_string()),
    ///     true
    /// );
    /// ```
    pub fn new(root: PathBuf, concept: Option<String>, enable_rbac: bool) -> Self {
        let permission_checker = PermissionChecker::new(root.clone());

        // Create default filesystem driver (concept-agnostic)
        let driver = Arc::new(FileSystemDriver::new(root.clone(), String::new())) as Arc<dyn StorageDriver>;

        Self {
            root,
            concept,
            enable_rbac,
            ontology: None,
            permission_checker,
            driver,
        }
    }

    /// Create a new Kernel instance with custom driver
    ///
    /// # Arguments
    ///
    /// * `root` - Root directory for concepts
    /// * `concept` - Optional kernel name (e.g., "Recipes.BakeCake")
    /// * `enable_rbac` - Enable RBAC checks (default: true)
    /// * `driver` - Custom storage driver implementation
    ///
    /// # Example
    ///
    /// ```
    /// use ckp_core::kernel::Kernel;
    /// use ckp_core::drivers::FileSystemDriver;
    /// use std::path::PathBuf;
    /// use std::sync::Arc;
    ///
    /// let root = PathBuf::from("/concepts");
    /// let driver = Arc::new(FileSystemDriver::new(root.clone()));
    /// let kernel = Kernel::with_driver(
    ///     root,
    ///     Some("Recipes.BakeCake".to_string()),
    ///     true,
    ///     driver
    /// );
    /// ```
    pub fn with_driver(
        root: PathBuf,
        concept: Option<String>,
        enable_rbac: bool,
        driver: Arc<dyn StorageDriver>,
    ) -> Self {
        let permission_checker = PermissionChecker::new(root.clone());

        Self {
            root,
            concept,
            enable_rbac,
            ontology: None,
            permission_checker,
            driver,
        }
    }

    /// Bootstrap kernel by loading ontology
    ///
    /// # Arguments
    ///
    /// * `concept_name` - Kernel name to bootstrap
    ///
    /// # Returns
    ///
    /// Result with ontology loaded
    pub async fn bootstrap(&mut self, concept_name: &str) -> Result<()> {
        // Strip domain prefix if present (e.g., "Org.ConceptKernel:Recipes.BakeCake")
        let kernel_name = if concept_name.contains(':') {
            concept_name.split(':').nth(1).unwrap_or(concept_name)
        } else {
            concept_name
        };

        self.concept = Some(kernel_name.to_string());

        // Load ontology
        let ontology_reader = OntologyReader::new(self.root.clone());
        match ontology_reader.read_by_kernel_name(kernel_name) {
            Ok(ontology) => {
                self.ontology = Some(ontology);
                println!("[Kernel] Bootstrapped {}", kernel_name);
                Ok(())
            }
            Err(e) => {
                eprintln!("[Kernel] Warning: Failed to load ontology for {}: {}", kernel_name, e);
                // Don't fail - kernel can still emit without ontology
                Ok(())
            }
        }
    }

    /// Auto-discover kernel from current binary location
    ///
    /// This method enables Rust binaries to automatically discover:
    /// - Project root directory
    /// - Kernel name from binary location
    /// - Ontology and configuration
    ///
    /// Expected binary location patterns:
    /// - `/path/to/project/concepts/KernelName/tool/rs/binary`
    /// - `/path/to/project/concepts/KernelName/tool/binary`
    ///
    /// # Returns
    ///
    /// Initialized Kernel instance with:
    /// - `root` set to project root
    /// - `concept` set to kernel name
    /// - `ontology` loaded from conceptkernel.yaml
    ///
    /// # Errors
    ///
    /// Returns error if:
    /// - Cannot determine current executable path
    /// - Binary not in expected location structure
    /// - Cannot find project root or concepts directory
    ///
    /// # Example
    ///
    /// ```no_run
    /// # use ckp_core::kernel::Kernel;
    /// # async fn example() -> ckp_core::errors::Result<()> {
    /// // Binary at: /project/concepts/System.Gateway.HTTP/tool/rs/gateway-http
    /// let kernel = Kernel::from_current_binary().await?;
    /// // kernel.root = "/project"
    /// // kernel.concept = Some("System.Gateway.HTTP")
    /// # Ok(())
    /// # }
    /// ```
    pub async fn from_current_binary() -> Result<Self> {
        // Get current executable path
        let exe_path = std::env::current_exe()
            .map_err(|e| CkpError::IoError(format!("Failed to get executable path: {}", e)))?;

        println!("[Kernel] Auto-discovering from binary: {}", exe_path.display());

        // Navigate up directory tree to find concepts directory
        let mut current = exe_path.parent();
        let mut kernel_name = None;
        let mut project_root = None;

        // Expected structure: .../concepts/KernelName/tool/rs/binary or .../concepts/KernelName/tool/binary
        while let Some(dir) = current {
            // Check if we're in concepts/KernelName/tool/... structure
            if let Some(parent) = dir.parent() {
                if let Some(grandparent) = parent.parent() {
                    if grandparent.file_name().and_then(|n| n.to_str()) == Some("concepts") {
                        // Found: concepts/KernelName/tool
                        kernel_name = parent.file_name().and_then(|n| n.to_str()).map(|s| s.to_string());
                        project_root = grandparent.parent().map(|p| p.to_path_buf());
                        break;
                    }
                }
            }

            current = dir.parent();
        }

        let kernel_name = kernel_name.ok_or_else(|| {
            CkpError::ParseError(format!(
                "Could not determine kernel name from binary location: {}. Expected structure: .../concepts/KernelName/tool/rs/binary",
                exe_path.display()
            ))
        })?;

        let root = project_root.ok_or_else(|| {
            CkpError::ParseError(format!(
                "Could not determine project root from binary location: {}. Expected structure: .../concepts/KernelName/tool/rs/binary",
                exe_path.display()
            ))
        })?;

        println!("[Kernel] Discovered kernel: {} at root: {}", kernel_name, root.display());

        // Create kernel instance and bootstrap
        let mut kernel = Kernel::new(root, Some(kernel_name.clone()), true);
        kernel.bootstrap(&kernel_name).await?;

        Ok(kernel)
    }

    /// Get or allocate port for this kernel using PortManager
    ///
    /// This method reads the port allocation from the project's `.ckports` file.
    /// If no port is allocated, it dynamically allocates one within the project's port range.
    ///
    /// # Returns
    ///
    /// Port number for this kernel
    ///
    /// # Errors
    ///
    /// Returns error if:
    /// - Kernel concept name not set (call `bootstrap()` first)
    /// - Project base port not configured (run `ckr project init` first)
    /// - No available ports in range
    ///
    /// # Example
    ///
    /// ```no_run
    /// # use ckp_core::kernel::Kernel;
    /// # async fn example() -> ckp_core::errors::Result<()> {
    /// let kernel = Kernel::from_current_binary().await?;
    /// let port = kernel.get_or_allocate_port()?;
    /// println!("Using port: {}", port);
    /// # Ok(())
    /// # }
    /// ```
    pub fn get_or_allocate_port(&self) -> Result<u16> {
        let kernel_name = self.concept.as_ref().ok_or_else(|| {
            CkpError::PortError("Kernel concept not set. Call bootstrap() first.".to_string())
        })?;

        let mut port_manager = PortManager::new(&self.root)?;

        // Try to get existing allocation first
        if let Some(port) = port_manager.get(kernel_name) {
            println!("[Kernel] Using existing port allocation: {}", port);
            return Ok(port);
        }

        // Allocate new port
        let port = port_manager.allocate(kernel_name, None)?;
        println!("[Kernel] Allocated new port: {}", port);

        Ok(port)
    }

    /// Get project root directory
    pub fn root(&self) -> &PathBuf {
        &self.root
    }

    /// Get kernel concept name
    pub fn concept_name(&self) -> Option<&str> {
        self.concept.as_deref()
    }

    /// Get loaded ontology
    pub fn ontology(&self) -> Option<&Ontology> {
        self.ontology.as_ref()
    }

    /// Iterate over jobs in inbox
    ///
    /// Returns an iterator over all `.job` files in the kernel's inbox directory.
    /// Each job can be processed and then archived using `job.archive()`.
    ///
    /// # Returns
    ///
    /// InboxIterator that yields Result<Job>
    ///
    /// # Errors
    ///
    /// Returns error if:
    /// - Kernel concept name not set
    /// - Inbox directory doesn't exist or can't be read
    ///
    /// # Example
    ///
    /// ```no_run
    /// # use ckp_core::kernel::Kernel;
    /// # async fn example() -> ckp_core::errors::Result<()> {
    /// let kernel = Kernel::from_current_binary().await?;
    ///
    /// for job_result in kernel.inbox_iter()? {
    ///     let job = job_result?;
    ///     println!("Processing job: {}", job.tx_id());
    ///
    ///     // Process the payload
    ///     let payload = job.payload();
    ///     // ... do work ...
    ///
    ///     // Archive when done
    ///     job.archive()?;
    /// }
    /// # Ok(())
    /// # }
    /// ```
    pub fn inbox_iter(&self) -> Result<InboxIterator> {
        let kernel_name = self.concept.as_ref().ok_or_else(|| {
            CkpError::ParseError("Kernel concept not set. Call bootstrap() first.".to_string())
        })?;

        let inbox_dir = self.root.join("concepts").join(kernel_name).join("queue/inbox");
        let archive_dir = self.root.join("concepts").join(kernel_name).join("archive");

        // Ensure archive directory exists
        if !archive_dir.exists() {
            fs::create_dir_all(&archive_dir)
                .map_err(|e| CkpError::IoError(format!("Failed to create archive directory: {}", e)))?;
        }

        // Check if inbox exists
        if !inbox_dir.exists() {
            return Ok(InboxIterator {
                jobs: Vec::new(),
                index: 0,
                archive_dir,
            });
        }

        // Read all .job files from inbox
        let mut jobs = Vec::new();
        for entry in fs::read_dir(&inbox_dir)
            .map_err(|e| CkpError::IoError(format!("Failed to read inbox directory: {}", e)))?
        {
            let entry = entry
                .map_err(|e| CkpError::IoError(format!("Failed to read directory entry: {}", e)))?;

            let path = entry.path();
            if path.extension().and_then(|s| s.to_str()) == Some("job") {
                jobs.push(path);
            }
        }

        // Sort by filename (which includes timestamp) for deterministic ordering
        jobs.sort();

        println!("[Kernel] Found {} jobs in inbox", jobs.len());

        Ok(InboxIterator {
            jobs,
            index: 0,
            archive_dir,
        })
    }

    /// Emit a job to a target kernel with RBAC checks
    ///
    /// # Arguments
    ///
    /// * `target` - Target kernel name or full URN
    ///   - Simple name: "Recipes.BakeCake"
    ///   - Full URN: "ckp://Recipes.BakeCake:v0.1"
    ///   - URN with stage: "ckp://Recipes.BakeCake:v0.1#inbox"
    /// * `payload` - Job payload data (any valid JSON)
    ///
    /// # Returns
    ///
    /// Transaction ID in format "{timestamp}-{shortId}"
    ///
    /// # Errors
    ///
    /// Returns error if:
    /// - RBAC check fails (communication not authorized)
    /// - URN parsing fails (invalid format)
    /// - File system operations fail (permission denied, disk full)
    ///
    /// # Example
    ///
    /// ```no_run
    /// # use ckp_core::kernel::Kernel;
    /// # use std::path::PathBuf;
    /// # async fn example() -> ckp_core::errors::Result<()> {
    /// let mut kernel = Kernel::new(
    ///     PathBuf::from("/concepts"),
    ///     Some("Recipes.MixIngredients".to_string()),
    ///     true
    /// );
    /// kernel.bootstrap("Recipes.MixIngredients").await?;
    ///
    /// // Emit with simple kernel name
    /// let tx_id = kernel.emit("Recipes.BakeCake", serde_json::json!({"data": "test"})).await?;
    ///
    /// // Emit with full URN
    /// let tx_id = kernel.emit("ckp://Recipes.BakeCake:v0.1", serde_json::json!({"data": "test"})).await?;
    /// # Ok(())
    /// # }
    /// ```
    pub async fn emit(&mut self, target: &str, payload: serde_json::Value) -> Result<String> {
        // ===== STEP 1: RBAC AUTHORIZATION CHECK =====
        if self.enable_rbac && self.concept.is_some() {
            let source_urn = self.construct_source_urn();
            let target_urn = self.normalize_target_urn(target);

            // Check authorization (throws on denial)
            self.permission_checker.assert_can_emit_to(&source_urn, &target_urn)?;
        }

        // ===== STEP 2: TRANSACTION ID GENERATION =====
        let tx_id = self.generate_tx_id();

        // ===== STEP 3: JOB CONTENT CREATION =====
        let timestamp = Utc::now().to_rfc3339();
        let source = self.concept.as_ref()
            .map(|s| s.clone())
            .unwrap_or_else(|| "external".to_string());

        let job = DriverJobFile {
            target: target.to_string(),
            payload,
            timestamp,
            tx_id: tx_id.clone(),
            source,
        };

        // ===== STEP 4: WRITE JOB VIA DRIVER =====
        // Driver abstracts storage backend (filesystem, S3, Redis, etc.)
        let returned_tx_id = self.driver.write_job(target, job)?;

        // ===== STEP 5: LOGGING AND RETURN =====
        println!("[Kernel] Emitted job {} to {}", returned_tx_id, target);

        Ok(returned_tx_id)
    }

    // ===== PHASE 1: CORE KERNEL API METHODS =====
    // These methods enable concept kernels to operate through Kernel API
    // abstraction instead of directly accessing the filesystem.

    /// Get another kernel instance by name
    ///
    /// This allows concept kernels to interact with other kernels through
    /// the Kernel API without directly accessing the filesystem.
    ///
    /// # Arguments
    ///
    /// * `name` - Kernel name (e.g., "System.Registry", "Recipes.BakeCake")
    ///
    /// # Returns
    ///
    /// A new Kernel instance for the specified kernel with ontology loaded
    ///
    /// # Errors
    ///
    /// Returns error if:
    /// - Kernel directory doesn't exist
    /// - Failed to initialize kernel
    /// - Failed to load ontology
    ///
    /// # Example
    ///
    /// ```no_run
    /// # use ckp_core::kernel::Kernel;
    /// # async fn example() -> ckp_core::errors::Result<()> {
    /// let kernel = Kernel::from_current_binary().await?;
    ///
    /// // Get another kernel instance by name
    /// let registry = kernel.get_kernel_by_name("System.Registry")?;
    /// let registry_ontology = registry.ontology();
    /// # Ok(())
    /// # }
    /// ```
    pub fn get_kernel_by_name(&self, name: &str) -> Result<Kernel> {
        // Verify kernel directory exists
        let kernel_path = self.root.join("concepts").join(name);
        if !kernel_path.exists() {
            return Err(CkpError::FileNotFound(format!(
                "Kernel not found: {}. Expected directory: {}",
                name,
                kernel_path.display()
            )));
        }

        // Create new Kernel instance for the target kernel with same RBAC setting
        let mut kernel = Kernel::new(
            self.root.clone(),
            Some(name.to_string()),
            self.enable_rbac,
        );

        // Bootstrap to load ontology (non-blocking if ontology missing)
        // We use a runtime block to call async bootstrap
        let rt = tokio::runtime::Handle::try_current()
            .or_else(|_| {
                // If no runtime, create a new one
                tokio::runtime::Runtime::new().map(|rt| rt.handle().clone())
            })
            .map_err(|e| CkpError::IoError(format!("Failed to get tokio runtime: {}", e)))?;

        rt.block_on(kernel.bootstrap(name))?;

        Ok(kernel)
    }

    /// Load or reload ontology for the current kernel
    ///
    /// This method loads the conceptkernel.yaml file for the current kernel.
    /// If already loaded, it reloads the ontology from disk.
    ///
    /// # Returns
    ///
    /// Reference to the loaded Ontology
    ///
    /// # Errors
    ///
    /// Returns error if:
    /// - Kernel concept name not set (call `bootstrap()` first)
    /// - Ontology file doesn't exist
    /// - Failed to parse ontology
    ///
    /// # Example
    ///
    /// ```no_run
    /// # use ckp_core::kernel::Kernel;
    /// # async fn example() -> ckp_core::errors::Result<()> {
    /// let mut kernel = Kernel::from_current_binary().await?;
    ///
    /// // Load or reload ontology
    /// let ontology = kernel.load_ontology()?;
    /// println!("Loaded ontology: {}", ontology.metadata.get_urn());
    /// # Ok(())
    /// # }
    /// ```
    pub fn load_ontology(&mut self) -> Result<&Ontology> {
        let kernel_name = self.concept.as_ref().ok_or_else(|| {
            CkpError::ParseError("Kernel concept not set. Call bootstrap() first.".to_string())
        })?;

        // Load ontology using OntologyReader
        let ontology_reader = OntologyReader::new(self.root.clone());
        let ontology = ontology_reader.read_by_kernel_name(kernel_name)
            .map_err(|e| CkpError::ParseError(format!(
                "Failed to load ontology for {}: {}",
                kernel_name, e
            )))?;

        self.ontology = Some(ontology);

        // Safe to unwrap because we just set it
        Ok(self.ontology.as_ref().unwrap())
    }

    /// Update ontology by writing changes to conceptkernel.yaml
    ///
    /// This method updates the kernel's conceptkernel.yaml file with the provided changes.
    /// The changes are merged with the existing ontology document.
    ///
    /// # Arguments
    ///
    /// * `changes` - JSON object containing ontology changes (partial or full document)
    ///
    /// # Returns
    ///
    /// Success or error
    ///
    /// # Errors
    ///
    /// Returns error if:
    /// - Kernel concept name not set
    /// - Failed to serialize changes to YAML
    /// - Failed to write to file system
    ///
    /// # Example
    ///
    /// ```no_run
    /// # use ckp_core::kernel::Kernel;
    /// # async fn example() -> ckp_core::errors::Result<()> {
    /// let mut kernel = Kernel::from_current_binary().await?;
    ///
    /// // Update ontology metadata
    /// let changes = serde_json::json!({
    ///     "metadata": {
    ///         "description": "Updated description"
    ///     }
    /// });
    ///
    /// kernel.update_ontology(&changes)?;
    /// # Ok(())
    /// # }
    /// ```
    pub fn update_ontology(&self, changes: &serde_json::Value) -> Result<()> {
        let kernel_name = self.concept.as_ref().ok_or_else(|| {
            CkpError::ParseError("Kernel concept not set. Call bootstrap() first.".to_string())
        })?;

        // Load current ontology
        let ontology_reader = OntologyReader::new(self.root.clone());
        let mut current = ontology_reader.read_by_kernel_name(kernel_name)
            .map_err(|e| CkpError::ParseError(format!(
                "Failed to load current ontology for {}: {}",
                kernel_name, e
            )))?;

        // Convert current ontology to JSON for merging
        let mut current_json = serde_json::to_value(&current)
            .map_err(|e| CkpError::Json(e))?;

        // Merge changes into current ontology
        merge_json(&mut current_json, changes);

        // Convert back to Ontology struct
        current = serde_json::from_value(current_json)
            .map_err(|e| CkpError::Json(e))?;

        // Write updated ontology to conceptkernel.yaml
        let ontology_path = self.root
            .join("concepts")
            .join(kernel_name)
            .join("conceptkernel.yaml");

        let yaml_content = serde_yaml::to_string(&current)
            .map_err(|e| CkpError::ParseError(format!(
                "Failed to serialize ontology to YAML: {}",
                e
            )))?;

        fs::write(&ontology_path, yaml_content)
            .map_err(|e| CkpError::IoError(format!(
                "Failed to write ontology to {}: {}",
                ontology_path.display(), e
            )))?;

        println!("[Kernel] Updated ontology for {}", kernel_name);
        Ok(())
    }

    /// Load instance data by URN
    ///
    /// This method loads instance data from the kernel's storage directory.
    /// The URN is resolved to the appropriate storage path and the instance
    /// data is read and parsed as JSON.
    ///
    /// # Arguments
    ///
    /// * `urn` - Instance URN (e.g., "ckp://System.Proof/instances/user-123")
    ///
    /// # Returns
    ///
    /// JSON value containing the instance data
    ///
    /// # Errors
    ///
    /// Returns error if:
    /// - URN format is invalid
    /// - Instance file doesn't exist
    /// - Failed to parse JSON
    ///
    /// # Example
    ///
    /// ```no_run
    /// # use ckp_core::kernel::Kernel;
    /// # async fn example() -> ckp_core::errors::Result<()> {
    /// let kernel = Kernel::from_current_binary().await?;
    ///
    /// // Load instance by URN
    /// let instance = kernel.load_instance("ckp://System.Oidc.User/storage/user-alice")?;
    /// println!("Loaded instance: {:?}", instance);
    /// # Ok(())
    /// # }
    /// ```
    pub fn load_instance(&self, urn: &str) -> Result<serde_json::Value> {
        // Parse URN to extract kernel name and path
        let (kernel_name, storage_path) = parse_instance_urn(urn)?;

        // Build full path to instance file
        let instance_path = self.root
            .join("concepts")
            .join(&kernel_name)
            .join("storage")
            .join(&storage_path);

        // Add .inst extension if not present
        let instance_path = if instance_path.extension().is_none() {
            instance_path.with_extension("inst")
        } else {
            instance_path
        };

        // Read instance file
        let content = fs::read_to_string(&instance_path)
            .map_err(|e| CkpError::FileNotFound(format!(
                "Failed to read instance {}: {}",
                urn, e
            )))?;

        // Parse JSON
        let instance: serde_json::Value = serde_json::from_str(&content)
            .map_err(|e| CkpError::Json(e))?;

        Ok(instance)
    }

    /// Save instance data by URN
    ///
    /// This method saves instance data to the kernel's storage directory.
    /// The URN is resolved to the appropriate storage path and the instance
    /// data is serialized to JSON and written to disk.
    ///
    /// # Arguments
    ///
    /// * `urn` - Instance URN (e.g., "ckp://System.Proof/instances/user-123")
    /// * `data` - JSON value containing the instance data
    ///
    /// # Returns
    ///
    /// Success or error
    ///
    /// # Errors
    ///
    /// Returns error if:
    /// - URN format is invalid
    /// - Failed to serialize JSON
    /// - Failed to write to file system
    ///
    /// # Example
    ///
    /// ```no_run
    /// # use ckp_core::kernel::Kernel;
    /// # async fn example() -> ckp_core::errors::Result<()> {
    /// let kernel = Kernel::from_current_binary().await?;
    ///
    /// // Save instance data
    /// let user_data = serde_json::json!({
    ///     "username": "alice",
    ///     "email": "alice@example.com"
    /// });
    ///
    /// kernel.save_instance("ckp://System.Oidc.User/storage/user-alice", &user_data)?;
    /// # Ok(())
    /// # }
    /// ```
    pub fn save_instance(&self, urn: &str, data: &serde_json::Value) -> Result<()> {
        // Parse URN to extract kernel name and path
        let (kernel_name, storage_path) = parse_instance_urn(urn)?;

        // Build full path to instance file
        let instance_path = self.root
            .join("concepts")
            .join(&kernel_name)
            .join("storage")
            .join(&storage_path);

        // Add .inst extension if not present
        let instance_path = if instance_path.extension().is_none() {
            instance_path.with_extension("inst")
        } else {
            instance_path
        };

        // Ensure parent directory exists
        if let Some(parent) = instance_path.parent() {
            fs::create_dir_all(parent)
                .map_err(|e| CkpError::IoError(format!(
                    "Failed to create storage directory: {}",
                    e
                )))?;
        }

        // Serialize to pretty JSON
        let content = serde_json::to_string_pretty(data)
            .map_err(|e| CkpError::Json(e))?;

        // Write instance file
        fs::write(&instance_path, content)
            .map_err(|e| CkpError::IoError(format!(
                "Failed to write instance {}: {}",
                urn, e
            )))?;

        println!("[Kernel] Saved instance: {}", urn);
        Ok(())
    }

    // ===== PRIVATE HELPER METHODS =====

    /// Construct source URN from ontology or concept name
    fn construct_source_urn(&self) -> String {
        // Priority 1: Use ontology URN
        if let Some(ref ontology) = self.ontology {
            return ontology.metadata.get_urn();
        }

        // Priority 2: Construct from concept name
        if let Some(ref concept) = self.concept {
            return format!("ckp://{}", concept);
        }

        // Fallback: external
        "ckp://external".to_string()
    }

    /// Normalize target to URN format for RBAC checks
    fn normalize_target_urn(&self, target: &str) -> String {
        if target.starts_with("ckp://") {
            target.to_string()
        } else {
            format!("ckp://{}", target)
        }
    }

    /// Generate transaction ID: {timestamp}-{8char_hex}
    fn generate_tx_id(&self) -> String {
        let timestamp = Utc::now().timestamp_millis();
        let short_id = self.generate_short_id();
        format!("{}-{}", timestamp, short_id)
    }

    /// Generate 8-character hex short ID
    fn generate_short_id(&self) -> String {
        use rand::Rng;
        let mut rng = rand::thread_rng();
        let random_value: u32 = rng.gen();
        format!("{:08x}", random_value)
    }
}

// ===== HELPER FUNCTIONS FOR KERNEL API =====

/// Recursively merge JSON objects
///
/// Merges `source` into `target`, replacing values for matching keys.
/// For nested objects, performs deep merge.
fn merge_json(target: &mut serde_json::Value, source: &serde_json::Value) {
    if let (Some(target_obj), Some(source_obj)) = (target.as_object_mut(), source.as_object()) {
        for (key, value) in source_obj {
            if let Some(target_value) = target_obj.get_mut(key) {
                // If both are objects, merge recursively
                if target_value.is_object() && value.is_object() {
                    merge_json(target_value, value);
                } else {
                    // Otherwise, replace the value
                    *target_value = value.clone();
                }
            } else {
                // Key doesn't exist in target, insert it
                target_obj.insert(key.clone(), value.clone());
            }
        }
    }
}

/// Parse instance URN to extract kernel name and storage path
///
/// Expected URN formats:
/// - `ckp://KernelName/storage/path/to/instance`
/// - `ckp://KernelName/path/to/instance` (assumes storage prefix)
/// - `KernelName/storage/path/to/instance` (missing ckp:// prefix)
///
/// # Arguments
///
/// * `urn` - Instance URN to parse
///
/// # Returns
///
/// Tuple of (kernel_name, storage_path)
///
/// # Errors
///
/// Returns error if URN format is invalid
fn parse_instance_urn(urn: &str) -> Result<(String, String)> {
    // Remove ckp:// prefix if present
    let urn = urn.strip_prefix("ckp://").unwrap_or(urn);

    // Split on first '/' to separate kernel name from path
    let parts: Vec<&str> = urn.splitn(2, '/').collect();
    if parts.len() != 2 {
        return Err(CkpError::ParseError(format!(
            "Invalid instance URN format: {}. Expected format: ckp://KernelName/storage/path",
            urn
        )));
    }

    let kernel_name = parts[0].to_string();
    let path = parts[1];

    // Strip "storage/" prefix if present (for backwards compatibility)
    let storage_path = path.strip_prefix("storage/")
        .unwrap_or(path)
        .to_string();

    Ok((kernel_name, storage_path))
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;
    use tempfile::TempDir;

    fn setup_test_kernel() -> (TempDir, Kernel) {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let kernel = Kernel::new(root, Some("TestKernel".to_string()), false);
        (temp_dir, kernel)
    }

    #[test]
    fn test_kernel_new() {
        let root = PathBuf::from("/concepts");
        let kernel = Kernel::new(root.clone(), Some("TestKernel".to_string()), true);

        assert_eq!(kernel.root, root);
        assert_eq!(kernel.concept, Some("TestKernel".to_string()));
        assert_eq!(kernel.enable_rbac, true);
    }

    #[test]
    fn test_generate_tx_id_format() {
        let (_temp, kernel) = setup_test_kernel();
        let tx_id = kernel.generate_tx_id();

        // Format: {timestamp}-{8char_hex}
        let parts: Vec<&str> = tx_id.split('-').collect();
        assert_eq!(parts.len(), 2);

        // Timestamp should be numeric
        let timestamp: i64 = parts[0].parse().unwrap();
        assert!(timestamp > 0);

        // Short ID should be 8 hex characters
        assert_eq!(parts[1].len(), 8);
        assert!(parts[1].chars().all(|c| c.is_ascii_hexdigit()));
    }

    #[test]
    fn test_generate_short_id() {
        let (_temp, kernel) = setup_test_kernel();
        let short_id = kernel.generate_short_id();

        assert_eq!(short_id.len(), 8);
        assert!(short_id.chars().all(|c| c.is_ascii_hexdigit()));
    }

    #[test]
    fn test_normalize_target_urn_simple_name() {
        let (_temp, kernel) = setup_test_kernel();
        let normalized = kernel.normalize_target_urn("Recipes.BakeCake");
        assert_eq!(normalized, "ckp://Recipes.BakeCake");
    }

    #[test]
    fn test_normalize_target_urn_full_urn() {
        let (_temp, kernel) = setup_test_kernel();
        let normalized = kernel.normalize_target_urn("ckp://Recipes.BakeCake:v0.1");
        assert_eq!(normalized, "ckp://Recipes.BakeCake:v0.1");
    }

    #[test]
    fn test_construct_source_urn_from_concept() {
        let (_temp, kernel) = setup_test_kernel();
        let source_urn = kernel.construct_source_urn();
        assert_eq!(source_urn, "ckp://TestKernel");
    }

    #[test]
    fn test_construct_source_urn_no_concept() {
        let root = PathBuf::from("/concepts");
        let kernel = Kernel::new(root, None, false);
        let source_urn = kernel.construct_source_urn();
        assert_eq!(source_urn, "ckp://external");
    }

    #[tokio::test]
    async fn test_emit_simple_kernel_name() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), false);

        let payload = serde_json::json!({"data": "test"});
        let tx_id = kernel.emit("TargetKernel", payload.clone()).await.unwrap();

        // Verify transaction ID format
        assert!(tx_id.contains('-'));
        let parts: Vec<&str> = tx_id.split('-').collect();
        assert_eq!(parts.len(), 2);

        // Verify job file was created (FileSystemDriver uses concepts/ prefix)
        let job_path = root.join("concepts/TargetKernel/queue/inbox").join(format!("{}.job", tx_id));
        assert!(job_path.exists());

        // Verify job content
        let job_content = fs::read_to_string(&job_path).unwrap();
        let job: JobFile = serde_json::from_str(&job_content).unwrap();

        assert_eq!(job.target, "TargetKernel");
        assert_eq!(job.payload, payload);
        assert_eq!(job.source, "SourceKernel");
        assert_eq!(job.tx_id, tx_id);
        assert!(!job.timestamp.is_empty());
    }

    #[tokio::test]
    async fn test_emit_full_urn() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), false);

        let payload = serde_json::json!({"data": "test"});
        let tx_id = kernel.emit("ckp://TargetKernel:v0.1", payload.clone()).await.unwrap();

        // Verify job file was created in default inbox
        let job_path = root.join("concepts/TargetKernel/queue/inbox").join(format!("{}.job", tx_id));
        assert!(job_path.exists());

        // Verify job content
        let job_content = fs::read_to_string(&job_path).unwrap();
        let job: JobFile = serde_json::from_str(&job_content).unwrap();

        assert_eq!(job.target, "ckp://TargetKernel:v0.1");
    }

    #[tokio::test]
    async fn test_emit_urn_with_stage() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), false);

        let payload = serde_json::json!({"data": "test"});
        let tx_id = kernel.emit("ckp://TargetKernel:v0.1#staging", payload.clone()).await.unwrap();

        // Verify job file was created in staging queue (driver uses concepts/ prefix)
        let job_path = root.join("concepts/TargetKernel/queue/staging").join(format!("{}.job", tx_id));
        assert!(job_path.exists());
    }

    #[tokio::test]
    async fn test_emit_creates_inbox_directory() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), false);

        let inbox_path = root.join("concepts/NewKernel/queue/inbox");
        assert!(!inbox_path.exists());

        let payload = serde_json::json!({"data": "test"});
        kernel.emit("NewKernel", payload).await.unwrap();

        assert!(inbox_path.exists());
        assert!(inbox_path.is_dir());
    }

    #[tokio::test]
    async fn test_emit_external_source() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), None, false); // No concept name

        let payload = serde_json::json!({"data": "test"});
        let tx_id = kernel.emit("TargetKernel", payload).await.unwrap();

        let job_path = root.join("concepts/TargetKernel/queue/inbox").join(format!("{}.job", tx_id));
        let job_content = fs::read_to_string(&job_path).unwrap();
        let job: JobFile = serde_json::from_str(&job_content).unwrap();

        assert_eq!(job.source, "external");
    }

    // ===== RBAC INTEGRATION TESTS =====

    fn setup_ontology(root: &PathBuf, kernel_name: &str, allowed: Vec<&str>, denied: Vec<&str>) {
        let ontology_content = format!(
            r#"apiVersion: conceptkernel/v1
kind: Ontology
metadata:
  name: ckp://{}:v0.1
  type: node:cold
  version: v0.1

spec:
  rbac:
    communication:
      allowed:
{}
      denied:
{}
"#,
            kernel_name,
            allowed
                .iter()
                .map(|urn| format!("        - {}", urn))
                .collect::<Vec<_>>()
                .join("\n"),
            denied
                .iter()
                .map(|urn| format!("        - {}", urn))
                .collect::<Vec<_>>()
                .join("\n")
        );

        // OntologyReader expects: {root}/concepts/{kernel}/conceptkernel.yaml
        let kernel_dir = root.join("concepts").join(kernel_name);
        fs::create_dir_all(&kernel_dir).unwrap();
        fs::write(kernel_dir.join("conceptkernel.yaml"), ontology_content).unwrap();
    }

    #[tokio::test]
    async fn test_emit_rbac_whitelist_allowed() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Setup source kernel with RBAC whitelist
        setup_ontology(&root, "SourceKernel", vec!["ckp://TargetKernel"], vec![]);

        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), true);
        kernel.bootstrap("SourceKernel").await.unwrap();

        // Should succeed - target is in whitelist
        let payload = serde_json::json!({"data": "test"});
        let result = kernel.emit("TargetKernel", payload).await;

        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_emit_rbac_whitelist_denied() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Setup source kernel with RBAC whitelist
        setup_ontology(&root, "SourceKernel", vec!["ckp://AllowedKernel"], vec![]);

        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), true);
        kernel.bootstrap("SourceKernel").await.unwrap();

        // Should fail - target is NOT in whitelist
        let payload = serde_json::json!({"data": "test"});
        let result = kernel.emit("DeniedKernel", payload).await;

        assert!(result.is_err());
        let err = result.unwrap_err();
        assert!(matches!(err, CkpError::Rbac(_)));
    }

    #[tokio::test]
    async fn test_emit_rbac_blacklist_denied() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Setup source kernel with RBAC blacklist
        setup_ontology(&root, "SourceKernel", vec![], vec!["ckp://External.*"]);

        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), true);
        kernel.bootstrap("SourceKernel").await.unwrap();

        // Should fail - target matches blacklist pattern
        let payload = serde_json::json!({"data": "test"});
        let result = kernel.emit("External.BadKernel", payload).await;

        assert!(result.is_err());
        let err = result.unwrap_err();
        assert!(matches!(err, CkpError::Rbac(_)));
    }

    #[tokio::test]
    async fn test_emit_rbac_promiscuous_mode() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Setup source kernel with NO RBAC restrictions
        setup_ontology(&root, "SourceKernel", vec![], vec![]);

        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), true);
        kernel.bootstrap("SourceKernel").await.unwrap();

        // Should succeed - promiscuous mode (no restrictions)
        let payload = serde_json::json!({"data": "test"});
        let result = kernel.emit("AnyKernel", payload).await;

        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_emit_rbac_wildcard_allowed() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Setup source kernel with wildcard whitelist
        setup_ontology(&root, "SourceKernel", vec!["ckp://System.*"], vec![]);

        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), true);
        kernel.bootstrap("SourceKernel").await.unwrap();

        // Should succeed - matches wildcard pattern
        let payload = serde_json::json!({"data": "test"});
        let result = kernel.emit("System.Proof", payload.clone()).await;

        assert!(result.is_ok());

        // Should fail - doesn't match wildcard pattern
        let result2 = kernel.emit("Recipes.BakeCake", payload).await;
        assert!(result2.is_err());
    }

    #[tokio::test]
    async fn test_emit_rbac_disabled() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Setup source kernel with restrictive RBAC
        setup_ontology(&root, "SourceKernel", vec!["ckp://AllowedKernel"], vec![]);

        // Create kernel with RBAC DISABLED
        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), false);
        kernel.bootstrap("SourceKernel").await.unwrap();

        // Should succeed even though target not in whitelist - RBAC disabled
        let payload = serde_json::json!({"data": "test"});
        let result = kernel.emit("DeniedKernel", payload).await;

        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_emit_rbac_no_concept() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Create kernel with NO concept name (external emit)
        let mut kernel = Kernel::new(root.clone(), None, true);

        // Should succeed - RBAC skipped when concept is None
        let payload = serde_json::json!({"data": "test"});
        let result = kernel.emit("AnyKernel", payload).await;

        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_job_file_json_format() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), false);

        let payload = serde_json::json!({"nested": {"data": "test", "count": 42}});
        let tx_id = kernel.emit("TargetKernel", payload.clone()).await.unwrap();

        let job_path = root.join("concepts/TargetKernel/queue/inbox").join(format!("{}.job", tx_id));

        // Verify JSON is pretty-printed
        let job_content = fs::read_to_string(&job_path).unwrap();
        assert!(job_content.contains("  ")); // Should have indentation
        assert!(job_content.contains("\n")); // Should have newlines

        // Verify all fields are present
        assert!(job_content.contains("\"target\""));
        assert!(job_content.contains("\"payload\""));
        assert!(job_content.contains("\"timestamp\""));
        assert!(job_content.contains("\"txId\""));
        assert!(job_content.contains("\"source\""));
    }

    // ==================== PHASE 3: BOOTSTRAP & EMIT ERROR HANDLING (+11 TESTS) ====================

    // ----- Bootstrap Failure Scenarios (+6 tests) -----

    /// Test: Bootstrap with missing ontology file
    #[tokio::test]
    async fn test_bootstrap_with_missing_ontology() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), None, false);

        // Create kernel directory but no conceptkernel.yaml
        let kernel_dir = root.join("MissingOntology");
        fs::create_dir_all(&kernel_dir).unwrap();

        // Should not fail - gracefully handles missing ontology
        let result = kernel.bootstrap("MissingOntology").await;
        assert!(result.is_ok(), "Bootstrap should succeed without ontology");
        assert!(kernel.concept.is_some());
    }

    /// Test: Bootstrap with invalid YAML
    #[tokio::test]
    async fn test_bootstrap_with_invalid_ontology_yaml() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), None, false);

        // Create kernel directory with malformed YAML
        let kernel_dir = root.join("InvalidOntology");
        fs::create_dir_all(&kernel_dir).unwrap();

        let invalid_yaml = "this is not: valid: yaml: [[[";
        fs::write(kernel_dir.join("conceptkernel.yaml"), invalid_yaml).unwrap();

        // Should not fail - gracefully handles parse errors
        let result = kernel.bootstrap("InvalidOntology").await;
        assert!(result.is_ok(), "Bootstrap should handle invalid YAML gracefully");
    }

    /// Test: Bootstrap with missing kernel directory
    #[tokio::test]
    async fn test_bootstrap_with_missing_directory() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), None, false);

        // Try to bootstrap non-existent kernel
        let result = kernel.bootstrap("NonExistentKernel").await;

        // Should still succeed - kernel is lenient
        assert!(result.is_ok());
        assert_eq!(kernel.concept, Some("NonExistentKernel".to_string()));
    }

    /// Test: Bootstrap idempotency
    #[tokio::test]
    async fn test_bootstrap_idempotency() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), None, false);

        // Create valid kernel
        let kernel_dir = root.join("IdempotentKernel");
        fs::create_dir_all(&kernel_dir).unwrap();
        let ontology = r#"
apiVersion: conceptkernel/v1
kind: Ontology
metadata:
  name: ckp://IdempotentKernel:v0.1
  type: node:cold
  version: v0.1
"#;
        fs::write(kernel_dir.join("conceptkernel.yaml"), ontology).unwrap();

        // Bootstrap multiple times
        let result1 = kernel.bootstrap("IdempotentKernel").await;
        let result2 = kernel.bootstrap("IdempotentKernel").await;
        let result3 = kernel.bootstrap("IdempotentKernel").await;

        assert!(result1.is_ok());
        assert!(result2.is_ok());
        assert!(result3.is_ok());

        // Concept should remain set
        assert_eq!(kernel.concept, Some("IdempotentKernel".to_string()));
    }

    /// Test: Bootstrap with version in concept name
    #[tokio::test]
    async fn test_bootstrap_strips_version_from_concept() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), None, false);

        let kernel_dir = root.join("VersionedKernel");
        fs::create_dir_all(&kernel_dir).unwrap();

        // Bootstrap with version tag
        let result = kernel.bootstrap("VersionedKernel:v0.1").await;

        assert!(result.is_ok());
        // Should strip version and use base name
        assert_eq!(kernel.concept, Some("v0.1".to_string()));
    }

    /// Test: Bootstrap with domain prefix
    #[tokio::test]
    async fn test_bootstrap_with_domain_prefix() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), None, false);

        let kernel_dir = root.join("DomainKernel");
        fs::create_dir_all(&kernel_dir).unwrap();

        // Bootstrap with domain prefix (e.g., "Org.ConceptKernel:Recipes.BakeCake")
        let result = kernel.bootstrap("Org.ConceptKernel:DomainKernel").await;

        assert!(result.is_ok());
        // Should strip domain prefix and use kernel name after colon
        assert_eq!(kernel.concept, Some("DomainKernel".to_string()));
    }

    // ----- Emit Error Handling (+5 tests) -----

    /// Test: Emit to nonexistent target (should create directory)
    #[tokio::test]
    async fn test_emit_to_nonexistent_target() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), false);

        let payload = serde_json::json!({"data": "test"});

        // Emit to kernel that doesn't exist yet
        let result = kernel.emit("NewTargetKernel", payload).await;

        // Should succeed - creates inbox directory automatically
        assert!(result.is_ok());

        // Verify directory was created
        let inbox_path = root.join("concepts/NewTargetKernel/queue/inbox");
        assert!(inbox_path.exists(), "Inbox directory should be auto-created");
    }

    /// Test: Emit with nested payload
    #[tokio::test]
    async fn test_emit_with_complex_payload() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), false);

        // Complex nested payload with arrays and objects
        let payload = serde_json::json!({
            "nested": {
                "array": [1, 2, 3],
                "object": {"key": "value"},
                "null_field": null,
                "unicode": "Hello  "
            }
        });

        let result = kernel.emit("TargetKernel", payload.clone()).await;

        assert!(result.is_ok());
        let tx_id = result.unwrap();

        // Verify payload was preserved
        let job_path = root.join("concepts/TargetKernel/queue/inbox").join(format!("{}.job", tx_id));
        let job_content = fs::read_to_string(&job_path).unwrap();
        let job: JobFile = serde_json::from_str(&job_content).unwrap();

        assert_eq!(job.payload, payload);
    }

    /// Test: Emit with empty payload
    #[tokio::test]
    async fn test_emit_with_empty_payload() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), false);

        let payload = serde_json::json!({});
        let result = kernel.emit("TargetKernel", payload).await;

        assert!(result.is_ok(), "Should handle empty payload");
    }

    /// Test: Emit with null payload
    #[tokio::test]
    async fn test_emit_with_null_payload() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), false);

        let payload = serde_json::Value::Null;
        let result = kernel.emit("TargetKernel", payload).await;

        assert!(result.is_ok(), "Should handle null payload");
    }

    /// Test: Emit with large payload (10KB+)
    #[tokio::test]
    async fn test_emit_with_large_payload() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), false);

        // Create large payload (>10KB)
        let large_string = "A".repeat(12000);
        let payload = serde_json::json!({
            "large_field": large_string,
            "metadata": {"size": "12KB"}
        });

        let result = kernel.emit("TargetKernel", payload.clone()).await;

        assert!(result.is_ok(), "Should handle large payloads");

        // Verify payload was written correctly
        let tx_id = result.unwrap();
        let job_path = root.join("concepts/TargetKernel/queue/inbox").join(format!("{}.job", tx_id));
        let job_content = fs::read_to_string(&job_path).unwrap();
        let job: JobFile = serde_json::from_str(&job_content).unwrap();

        assert_eq!(job.payload, payload);
    }

    // ==================== PERFORMANCE BENCHMARK TESTS (+7 TESTS) ====================
    //
    // Throughput Testing: High-frequency emissions and concurrent load scenarios
    // These tests measure system performance under stress conditions

    /// Test: High-frequency emissions - 1000 emissions per second (Test 1/7)
    #[tokio::test]
    async fn test_perf_high_frequency_emissions_1000_per_sec() {
        use std::time::Instant;

        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("PerfSource".to_string()), false);

        // Create target directory
        let target_dir = root.join("concepts/PerfTarget/queue/inbox");
        fs::create_dir_all(&target_dir).unwrap();

        let emission_count = 1000;
        let start = Instant::now();

        // Emit 1000 jobs
        for i in 0..emission_count {
            let payload = serde_json::json!({"index": i});
            kernel.emit("PerfTarget", payload).await.unwrap();
        }

        let duration = start.elapsed();
        let emissions_per_sec = emission_count as f64 / duration.as_secs_f64();

        println!("[PERF] Emitted {} jobs in {:?} ({:.2} jobs/sec)",
                 emission_count, duration, emissions_per_sec);

        // Assert performance threshold: should handle at least 500 emissions/sec
        assert!(emissions_per_sec >= 500.0,
                "Expected at least 500 emissions/sec, got {:.2}", emissions_per_sec);

        // Verify all jobs were created
        let job_files: Vec<_> = fs::read_dir(&target_dir)
            .unwrap()
            .filter_map(|e| e.ok())
            .filter(|e| e.path().extension().and_then(|s| s.to_str()) == Some("job"))
            .collect();
        assert_eq!(job_files.len(), emission_count);
    }

    /// Test: High-frequency emissions - stress test with 2000 emissions (Test 2/7)
    #[tokio::test]
    async fn test_perf_high_frequency_emissions_stress() {
        use std::time::Instant;

        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("StressSource".to_string()), false);

        let target_dir = root.join("concepts/StressTarget/queue/inbox");
        fs::create_dir_all(&target_dir).unwrap();

        let emission_count = 2000;
        let start = Instant::now();

        for i in 0..emission_count {
            let payload = serde_json::json!({"batch": i / 100, "index": i % 100});
            kernel.emit("StressTarget", payload).await.unwrap();
        }

        let duration = start.elapsed();
        let emissions_per_sec = emission_count as f64 / duration.as_secs_f64();

        println!("[PERF] Stress test: {} jobs in {:?} ({:.2} jobs/sec)",
                 emission_count, duration, emissions_per_sec);

        // Should maintain at least 400 emissions/sec under stress
        assert!(emissions_per_sec >= 400.0,
                "Stress test failed: expected >=400 jobs/sec, got {:.2}", emissions_per_sec);
    }

    /// Test: Concurrent multi-kernel load - 5 kernels emitting simultaneously (Test 3/7)
    #[tokio::test]
    async fn test_perf_concurrent_multi_kernel_load() {
        use std::time::Instant;

        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Create target
        let target_dir = root.join("concepts/ConcurrentTarget/queue/inbox");
        fs::create_dir_all(&target_dir).unwrap();

        let kernel_count = 5;
        let emissions_per_kernel = 200;
        let start = Instant::now();

        // Spawn concurrent kernels
        let mut handles = vec![];
        for k in 0..kernel_count {
            let root_clone = root.clone();
            let handle = tokio::spawn(async move {
                let mut kernel = Kernel::new(
                    root_clone,
                    Some(format!("ConcurrentSource{}", k)),
                    false
                );

                for i in 0..emissions_per_kernel {
                    let payload = serde_json::json!({"kernel": k, "index": i});
                    kernel.emit("ConcurrentTarget", payload).await.unwrap();
                }
            });
            handles.push(handle);
        }

        // Wait for all to complete
        for handle in handles {
            handle.await.unwrap();
        }

        let duration = start.elapsed();
        let total_emissions = kernel_count * emissions_per_kernel;
        let emissions_per_sec = total_emissions as f64 / duration.as_secs_f64();

        println!("[PERF] Concurrent load: {} kernels  {} = {} jobs in {:?} ({:.2} jobs/sec)",
                 kernel_count, emissions_per_kernel, total_emissions, duration, emissions_per_sec);

        // Should handle at least 300 jobs/sec with concurrent load
        assert!(emissions_per_sec >= 300.0,
                "Concurrent load failed: expected >=300 jobs/sec, got {:.2}", emissions_per_sec);

        // Verify all jobs created
        let job_count = fs::read_dir(&target_dir)
            .unwrap()
            .filter_map(|e| e.ok())
            .filter(|e| e.path().extension().and_then(|s| s.to_str()) == Some("job"))
            .count();
        assert_eq!(job_count, total_emissions);
    }

    /// Test: Concurrent multi-kernel with RBAC overhead (Test 4/7)
    #[tokio::test]
    async fn test_perf_concurrent_with_rbac() {
        use std::time::Instant;

        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Setup ontologies with RBAC
        for i in 0..3 {
            let source_dir = root.join("concepts").join(format!("RbacSource{}", i));
            fs::create_dir_all(&source_dir).unwrap();
            let ontology = format!(r#"
apiVersion: conceptkernel/v1
kind: Ontology
metadata:
  name: ckp://RbacSource{}:v0.1
  type: node:cold
  version: v0.1
spec:
  rbac:
    communication:
      allowed:
        - "ckp://RbacTarget"
"#, i);
            fs::write(source_dir.join("conceptkernel.yaml"), ontology).unwrap();
        }

        let target_dir = root.join("concepts/RbacTarget/queue/inbox");
        fs::create_dir_all(&target_dir).unwrap();

        let start = Instant::now();
        let mut handles = vec![];

        for i in 0..3 {
            let root_clone = root.clone();
            let handle = tokio::spawn(async move {
                let mut kernel = Kernel::new(
                    root_clone,
                    Some(format!("RbacSource{}", i)),
                    true // RBAC enabled
                );
                kernel.bootstrap(&format!("RbacSource{}", i)).await.unwrap();

                for j in 0..100 {
                    let payload = serde_json::json!({"kernel": i, "index": j});
                    kernel.emit("RbacTarget", payload).await.unwrap();
                }
            });
            handles.push(handle);
        }

        for handle in handles {
            handle.await.unwrap();
        }

        let duration = start.elapsed();
        let total_emissions = 300;
        let emissions_per_sec = total_emissions as f64 / duration.as_secs_f64();

        println!("[PERF] RBAC overhead: {} jobs with RBAC checks in {:?} ({:.2} jobs/sec)",
                 total_emissions, duration, emissions_per_sec);

        // Should handle at least 150 jobs/sec even with RBAC overhead
        assert!(emissions_per_sec >= 150.0,
                "RBAC overhead too high: expected >=150 jobs/sec, got {:.2}", emissions_per_sec);
    }

    /// Test: Queue saturation - handling backlog (Test 5/7)
    #[tokio::test]
    async fn test_perf_queue_saturation() {
        use std::time::Instant;

        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("SaturationSource".to_string()), false);

        let target_dir = root.join("concepts/SaturationTarget/queue/inbox");
        fs::create_dir_all(&target_dir).unwrap();

        // Create initial backlog
        for i in 0..500 {
            let job_path = target_dir.join(format!("backlog-{}.job", i));
            fs::write(&job_path, format!("{{\"backlog\": {}}}", i)).unwrap();
        }

        // Now emit additional jobs on top of backlog
        let start = Instant::now();
        let new_emissions = 500;

        for i in 0..new_emissions {
            let payload = serde_json::json!({"new": i});
            kernel.emit("SaturationTarget", payload).await.unwrap();
        }

        let duration = start.elapsed();
        let emissions_per_sec = new_emissions as f64 / duration.as_secs_f64();

        println!("[PERF] Queue saturation: {} new jobs added to 500-job backlog in {:?} ({:.2} jobs/sec)",
                 new_emissions, duration, emissions_per_sec);

        // Should maintain at least 300 jobs/sec even with queue saturation
        assert!(emissions_per_sec >= 300.0,
                "Queue saturation degraded performance: expected >=300 jobs/sec, got {:.2}", emissions_per_sec);

        // Verify total jobs
        let total_jobs = fs::read_dir(&target_dir).unwrap().count();
        assert!(total_jobs >= 1000, "Should have at least 1000 jobs total");
    }

    /// Test: Queue saturation recovery (Test 6/7)
    #[tokio::test]
    async fn test_perf_queue_saturation_recovery() {
        use std::time::Instant;

        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("RecoverySource".to_string()), false);

        let target_dir = root.join("concepts/RecoveryTarget/queue/inbox");
        fs::create_dir_all(&target_dir).unwrap();

        // Phase 1: Create massive backlog (1000 jobs)
        for i in 0..1000 {
            let job_path = target_dir.join(format!("backlog-{}.job", i));
            fs::write(&job_path, "{}").unwrap();
        }

        // Phase 2: Continue emitting and measure if system maintains performance
        let start = Instant::now();
        for i in 0..100 {
            let payload = serde_json::json!({"recovery_test": i});
            kernel.emit("RecoveryTarget", payload).await.unwrap();
        }
        let duration = start.elapsed();
        let emissions_per_sec = 100.0 / duration.as_secs_f64();

        println!("[PERF] Queue recovery: 100 jobs on 1000-job backlog in {:?} ({:.2} jobs/sec)",
                 duration, emissions_per_sec);

        // System should not degrade below 200 jobs/sec even with huge backlog
        assert!(emissions_per_sec >= 200.0,
                "System degraded under heavy backlog: expected >=200 jobs/sec, got {:.2}", emissions_per_sec);
    }

    /// Test: Batch processing efficiency (Test 7/7)
    #[tokio::test]
    async fn test_perf_batch_processing_efficiency() {
        use std::time::Instant;

        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("BatchSource".to_string()), false);

        let target_dir = root.join("concepts/BatchTarget/queue/inbox");
        fs::create_dir_all(&target_dir).unwrap();

        // Simulate batch processing: 10 batches of 100 jobs each
        let batch_count = 10;
        let jobs_per_batch = 100;
        let start = Instant::now();

        for batch in 0..batch_count {
            let batch_start = Instant::now();

            for i in 0..jobs_per_batch {
                let payload = serde_json::json!({
                    "batch_id": batch,
                    "index": i,
                    "timestamp": chrono::Utc::now().timestamp_millis()
                });
                kernel.emit("BatchTarget", payload).await.unwrap();
            }

            let batch_duration = batch_start.elapsed();
            println!("[PERF] Batch {} completed in {:?}", batch, batch_duration);
        }

        let total_duration = start.elapsed();
        let total_jobs = batch_count * jobs_per_batch;
        let avg_jobs_per_sec = total_jobs as f64 / total_duration.as_secs_f64();

        println!("[PERF] Batch processing: {} jobs in {} batches, total {:?} ({:.2} jobs/sec)",
                 total_jobs, batch_count, total_duration, avg_jobs_per_sec);

        // Batch processing should achieve at least 400 jobs/sec average
        assert!(avg_jobs_per_sec >= 400.0,
                "Batch processing inefficient: expected >=400 jobs/sec, got {:.2}", avg_jobs_per_sec);

        // Verify all jobs created
        let job_count = fs::read_dir(&target_dir)
            .unwrap()
            .filter_map(|e| e.ok())
            .filter(|e| e.path().extension().and_then(|s| s.to_str()) == Some("job"))
            .count();
        assert_eq!(job_count, total_jobs);
    }

    // ==================== PHASE 4: KERNEL POLISH TESTS (+7 TESTS) ====================

    // ----- Transaction Rollback (+4 tests) -----

    /// Test: Rollback when job write fails (read-only directory)
    #[tokio::test]
    async fn test_emit_rollback_on_write_failure() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), false);

        // Create inbox directory and make it read-only
        let inbox_path = root.join("concepts/TargetKernel/queue/inbox");
        fs::create_dir_all(&inbox_path).unwrap();

        #[cfg(unix)]
        {
            use std::os::unix::fs::PermissionsExt;
            let mut perms = fs::metadata(&inbox_path).unwrap().permissions();
            perms.set_mode(0o444); // Read-only
            fs::set_permissions(&inbox_path, perms).unwrap();
        }

        let payload = serde_json::json!({"data": "test"});
        let result = kernel.emit("TargetKernel", payload).await;

        // Should fail due to write permissions
        #[cfg(unix)]
        assert!(result.is_err(), "Should fail when directory is read-only");

        // Cleanup: restore permissions
        #[cfg(unix)]
        {
            use std::os::unix::fs::PermissionsExt;
            let mut perms = fs::metadata(&inbox_path).unwrap().permissions();
            perms.set_mode(0o755);
            fs::set_permissions(&inbox_path, perms).unwrap();
        }
    }

    /// Test: Rollback on RBAC permission denied
    #[tokio::test]
    async fn test_emit_rollback_on_permission_error() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Setup RBAC with strict whitelist
        setup_ontology(&root, "SourceKernel", vec!["ckp://AllowedKernel"], vec![]);

        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), true);
        kernel.bootstrap("SourceKernel").await.unwrap();

        let payload = serde_json::json!({"data": "test"});
        let result = kernel.emit("DeniedKernel", payload).await;

        // Should fail RBAC check before any file operations
        assert!(result.is_err());
        assert!(matches!(result.unwrap_err(), CkpError::Rbac(_)));

        // Verify no job file was created (rollback successful)
        let job_dir = root.join("concepts/DeniedKernel/queue/inbox");
        if job_dir.exists() {
            let entries = fs::read_dir(&job_dir).unwrap();
            assert_eq!(entries.count(), 0, "No job files should exist after RBAC failure");
        }
    }

    /// Test: Rollback on corrupted state (invalid ontology during emit)
    #[tokio::test]
    async fn test_bootstrap_rollback_on_corruption() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();
        let mut kernel = Kernel::new(root.clone(), None, false);

        // Create valid ontology
        let kernel_dir = root.join("CorruptableKernel");
        fs::create_dir_all(&kernel_dir).unwrap();
        let valid_ontology = r#"
apiVersion: conceptkernel/v1
kind: Ontology
metadata:
  name: ckp://CorruptableKernel:v0.1
  type: node:cold
  version: v0.1
"#;
        fs::write(kernel_dir.join("conceptkernel.yaml"), valid_ontology).unwrap();

        // Bootstrap successfully
        let result = kernel.bootstrap("CorruptableKernel").await;
        assert!(result.is_ok());

        // Corrupt the ontology file
        fs::write(kernel_dir.join("conceptkernel.yaml"), "corrupted: [[[").unwrap();

        // Re-bootstrap - should handle corruption gracefully
        let result2 = kernel.bootstrap("CorruptableKernel").await;
        assert!(result2.is_ok(), "Should handle corrupted ontology gracefully");

        // Concept should still be set (state preserved from first bootstrap)
        assert_eq!(kernel.concept, Some("CorruptableKernel".to_string()));
    }

    /// Test: Multi-step operation rollback (emit multiple targets, one fails)
    #[tokio::test]
    async fn test_multi_step_operation_rollback() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Setup RBAC: allow Target1, deny Target2
        setup_ontology(&root, "SourceKernel", vec!["ckp://Target1"], vec![]);

        let mut kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), true);
        kernel.bootstrap("SourceKernel").await.unwrap();

        let payload = serde_json::json!({"data": "test"});

        // First emit should succeed
        let result1 = kernel.emit("Target1", payload.clone()).await;
        assert!(result1.is_ok());

        // Second emit should fail (RBAC)
        let result2 = kernel.emit("Target2", payload.clone()).await;
        assert!(result2.is_err());

        // Verify first job exists
        let job1_dir = root.join("concepts/Target1/queue/inbox");
        assert!(job1_dir.exists());
        let entries1 = fs::read_dir(&job1_dir).unwrap();
        assert_eq!(entries1.count(), 1, "First job should exist");

        // Verify second job was never created (rollback)
        let job2_dir = root.join("concepts/Target2/queue/inbox");
        if job2_dir.exists() {
            let entries2 = fs::read_dir(&job2_dir).unwrap();
            assert_eq!(entries2.count(), 0, "Second job should not exist after RBAC failure");
        }
    }

    // ----- RBAC with Consensus (+3 tests) -----

    /// Test: Emit requiring consensus (simulate consensus requirement)
    #[tokio::test]
    async fn test_emit_with_consensus_requirement() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Setup ontology with consensus requirement (simulated via RBAC)
        let ontology_content = r#"
apiVersion: conceptkernel/v1
kind: Ontology
metadata:
  name: ckp://ConsensusKernel:v0.1
  type: node:cold
  version: v0.1

spec:
  rbac:
    communication:
      allowed:
        - ckp://Validator1
        - ckp://Validator2
      consensus:
        required: true
        threshold: 2
"#;
        let kernel_dir = root.join("concepts/ConsensusKernel");
        fs::create_dir_all(&kernel_dir).unwrap();
        fs::write(kernel_dir.join("conceptkernel.yaml"), ontology_content).unwrap();

        let mut kernel = Kernel::new(root.clone(), Some("ConsensusKernel".to_string()), true);
        kernel.bootstrap("ConsensusKernel").await.unwrap();

        // Note: Current implementation doesn't enforce consensus in emit()
        // This test verifies that ontology with consensus fields can be loaded
        let payload = serde_json::json!({"data": "test"});
        let result = kernel.emit("Validator1", payload).await;

        // Should succeed - consensus enforcement would be in EdgeKernel/routing layer
        assert!(result.is_ok(), "Should load ontology with consensus fields");
    }

    /// Test: Consensus timeout handling (simulated)
    #[tokio::test]
    async fn test_emit_consensus_timeout() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Setup ontology with consensus timeout
        let ontology_content = r#"
apiVersion: conceptkernel/v1
kind: Ontology
metadata:
  name: ckp://TimeoutKernel:v0.1
  type: node:cold
  version: v0.1

spec:
  rbac:
    communication:
      allowed:
        - ckp://Validator1
        - ckp://Validator2
        - ckp://Validator3
      consensus:
        required: true
        threshold: 3
        timeout_ms: 5000
"#;
        let kernel_dir = root.join("concepts/TimeoutKernel");
        fs::create_dir_all(&kernel_dir).unwrap();
        fs::write(kernel_dir.join("conceptkernel.yaml"), ontology_content).unwrap();

        let mut kernel = Kernel::new(root.clone(), Some("TimeoutKernel".to_string()), true);
        kernel.bootstrap("TimeoutKernel").await.unwrap();

        // Verify ontology loaded successfully with consensus timeout
        assert!(kernel.ontology.is_some());

        // Emit should succeed (consensus validation happens at routing layer)
        let payload = serde_json::json!({"data": "test"});
        let result = kernel.emit("Validator1", payload).await;
        assert!(result.is_ok(), "Should handle consensus timeout configuration");
    }

    /// Test: Partial consensus scenarios (simulated)
    #[tokio::test]
    async fn test_emit_partial_consensus() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Setup ontology with partial consensus (2 of 3)
        let ontology_content = r#"
apiVersion: conceptkernel/v1
kind: Ontology
metadata:
  name: ckp://PartialConsensusKernel:v0.1
  type: node:cold
  version: v0.1

spec:
  rbac:
    communication:
      allowed:
        - ckp://Validator1
        - ckp://Validator2
        - ckp://Validator3
      consensus:
        required: true
        threshold: 2
        total: 3
"#;
        let kernel_dir = root.join("concepts/PartialConsensusKernel");
        fs::create_dir_all(&kernel_dir).unwrap();
        fs::write(kernel_dir.join("conceptkernel.yaml"), ontology_content).unwrap();

        let mut kernel = Kernel::new(root.clone(), Some("PartialConsensusKernel".to_string()), true);
        kernel.bootstrap("PartialConsensusKernel").await.unwrap();

        // Emit to validators
        let payload = serde_json::json!({"data": "test"});
        let result1 = kernel.emit("Validator1", payload.clone()).await;
        let result2 = kernel.emit("Validator2", payload.clone()).await;

        // Both should succeed - partial consensus configuration accepted
        assert!(result1.is_ok(), "Should handle partial consensus (2/3)");
        assert!(result2.is_ok(), "Should handle partial consensus (2/3)");

        // Verify jobs were created
        assert!(root.join("concepts/Validator1/queue/inbox").exists());
        assert!(root.join("concepts/Validator2/queue/inbox").exists());
    }

    // ==================== PHASE 1: CORE KERNEL API TESTS (+15 TESTS) ====================
    //
    // Tests for new Kernel API methods that enable concept kernels to operate
    // through abstraction instead of directly accessing the filesystem.
    //
    // Methods tested:
    // 1. get_kernel_by_name() - Get another kernel instance by name
    // 2. load_ontology() - Load/reload ontology for current kernel
    // 3. update_ontology() - Update ontology with changes
    // 4. load_instance() - Load instance data by URN
    // 5. save_instance() - Save instance data by URN

    // ----- get_kernel_by_name() Tests (+3 tests) -----

    /// Test: Get existing kernel by name
    #[test]
    fn test_get_kernel_by_name_success() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Create target kernel directory with ontology
        let target_dir = root.join("concepts/TargetKernel");
        fs::create_dir_all(&target_dir).unwrap();

        let ontology_content = r#"
apiVersion: conceptkernel/v1
kind: Ontology
metadata:
  name: ckp://TargetKernel:v0.1
  type: node:cold
  version: v0.1
"#;
        fs::write(target_dir.join("conceptkernel.yaml"), ontology_content).unwrap();

        // Create source kernel
        let kernel = Kernel::new(root.clone(), Some("SourceKernel".to_string()), false);

        // Get target kernel by name
        let target = kernel.get_kernel_by_name("TargetKernel").unwrap();

        // Verify target kernel was initialized
        assert_eq!(target.concept_name(), Some("TargetKernel"));
        assert!(target.ontology().is_some());
    }

    /// Test: Get kernel by name - non-existent kernel
    #[test]
    fn test_get_kernel_by_name_not_found() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        let kernel = Kernel::new(root, Some("SourceKernel".to_string()), false);

        // Try to get non-existent kernel
        let result = kernel.get_kernel_by_name("NonExistentKernel");

        assert!(result.is_err());
        let err = result.err().unwrap();
        match err {
            CkpError::FileNotFound(ref msg) => {
                assert!(msg.contains("NonExistentKernel"));
                assert!(msg.contains("Kernel not found"));
            }
            _ => panic!("Expected FileNotFound error"),
        }
    }

    /// Test: Get kernel by name - multiple kernels
    #[test]
    fn test_get_kernel_by_name_multiple() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Create multiple kernel directories
        for name in &["Kernel1", "Kernel2", "Kernel3"] {
            let kernel_dir = root.join("concepts").join(name);
            fs::create_dir_all(&kernel_dir).unwrap();

            let ontology = format!(r#"
apiVersion: conceptkernel/v1
kind: Ontology
metadata:
  name: ckp://{}:v0.1
  type: node:cold
  version: v0.1
"#, name);
            fs::write(kernel_dir.join("conceptkernel.yaml"), ontology).unwrap();
        }

        let source = Kernel::new(root.clone(), Some("Source".to_string()), false);

        // Get each kernel successfully
        let k1 = source.get_kernel_by_name("Kernel1").unwrap();
        let k2 = source.get_kernel_by_name("Kernel2").unwrap();
        let k3 = source.get_kernel_by_name("Kernel3").unwrap();

        assert_eq!(k1.concept_name(), Some("Kernel1"));
        assert_eq!(k2.concept_name(), Some("Kernel2"));
        assert_eq!(k3.concept_name(), Some("Kernel3"));
    }

    // ----- load_ontology() Tests (+3 tests) -----

    /// Test: Load ontology for current kernel
    #[tokio::test]
    async fn test_load_ontology_success() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Create kernel directory with ontology
        let kernel_dir = root.join("concepts/TestKernel");
        fs::create_dir_all(&kernel_dir).unwrap();

        let ontology_content = r#"
apiVersion: conceptkernel/v1
kind: Ontology
metadata:
  name: ckp://TestKernel:v0.1
  type: rust:hot
  version: v0.1
  description: "Test kernel"
"#;
        fs::write(kernel_dir.join("conceptkernel.yaml"), ontology_content).unwrap();

        // Create kernel and bootstrap
        let mut kernel = Kernel::new(root.clone(), Some("TestKernel".to_string()), false);
        kernel.bootstrap("TestKernel").await.unwrap();

        // Load ontology
        let ontology = kernel.load_ontology().unwrap();

        // Verify ontology was loaded (URN format may vary)
        assert!(ontology.metadata.get_urn().contains("TestKernel"));
        assert_eq!(ontology.metadata.version, Some("v0.1".to_string()));
    }

    /// Test: Load ontology without bootstrap (should fail)
    #[test]
    fn test_load_ontology_no_concept() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Create kernel without concept name
        let mut kernel = Kernel::new(root, None, false);

        // Try to load ontology without bootstrapping
        let result = kernel.load_ontology();

        assert!(result.is_err());
        match result.unwrap_err() {
            CkpError::ParseError(msg) => {
                assert!(msg.contains("concept not set"));
            }
            _ => panic!("Expected ParseError"),
        }
    }

    /// Test: Reload ontology (changes reflected)
    #[tokio::test]
    async fn test_load_ontology_reload() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        let kernel_dir = root.join("concepts/ReloadKernel");
        fs::create_dir_all(&kernel_dir).unwrap();
        let ontology_path = kernel_dir.join("conceptkernel.yaml");

        // Write initial ontology
        let initial_ontology = r#"
apiVersion: conceptkernel/v1
kind: Ontology
metadata:
  name: ckp://ReloadKernel:v0.1
  type: node:cold
  version: v0.1
  description: "Initial version"
"#;
        fs::write(&ontology_path, initial_ontology).unwrap();

        let mut kernel = Kernel::new(root.clone(), Some("ReloadKernel".to_string()), false);
        kernel.bootstrap("ReloadKernel").await.unwrap();

        // Load initial ontology
        let ontology1 = kernel.load_ontology().unwrap();
        assert_eq!(ontology1.metadata.description.as_deref(), Some("Initial version"));

        // Update ontology file
        let updated_ontology = r#"
apiVersion: conceptkernel/v1
kind: Ontology
metadata:
  name: ckp://ReloadKernel:v0.1
  type: node:cold
  version: v0.1
  description: "Updated version"
"#;
        fs::write(&ontology_path, updated_ontology).unwrap();

        // Reload ontology
        let ontology2 = kernel.load_ontology().unwrap();
        assert_eq!(ontology2.metadata.description.as_deref(), Some("Updated version"));
    }

    // ----- update_ontology() Tests (+3 tests) -----

    /// Test: Update ontology metadata
    #[tokio::test]
    async fn test_update_ontology_success() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        let kernel_dir = root.join("concepts/UpdateKernel");
        fs::create_dir_all(&kernel_dir).unwrap();

        let initial_ontology = r#"
apiVersion: conceptkernel/v1
kind: Ontology
metadata:
  name: ckp://UpdateKernel:v0.1
  type: node:cold
  version: v0.1
  description: "Initial"
"#;
        fs::write(kernel_dir.join("conceptkernel.yaml"), initial_ontology).unwrap();

        let kernel = Kernel::new(root.clone(), Some("UpdateKernel".to_string()), false);

        // Update metadata description
        let changes = serde_json::json!({
            "metadata": {
                "description": "Updated description"
            }
        });

        kernel.update_ontology(&changes).unwrap();

        // Verify changes were written
        let content = fs::read_to_string(kernel_dir.join("conceptkernel.yaml")).unwrap();
        assert!(content.contains("Updated description"));
    }

    /// Test: Update ontology - deep merge
    #[tokio::test]
    async fn test_update_ontology_deep_merge() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        let kernel_dir = root.join("concepts/MergeKernel");
        fs::create_dir_all(&kernel_dir).unwrap();

        let initial_ontology = r#"
apiVersion: conceptkernel/v1
kind: Ontology
metadata:
  name: ckp://MergeKernel:v0.1
  type: node:cold
  version: v0.1
spec:
  rbac:
    communication:
      allowed:
        - ckp://Kernel1
"#;
        fs::write(kernel_dir.join("conceptkernel.yaml"), initial_ontology).unwrap();

        let kernel = Kernel::new(root.clone(), Some("MergeKernel".to_string()), false);

        // Add more allowed kernels (deep merge)
        let changes = serde_json::json!({
            "spec": {
                "rbac": {
                    "communication": {
                        "allowed": [
                            "ckp://Kernel1",
                            "ckp://Kernel2",
                            "ckp://Kernel3"
                        ]
                    }
                }
            }
        });

        kernel.update_ontology(&changes).unwrap();

        // Verify merge was successful
        let content = fs::read_to_string(kernel_dir.join("conceptkernel.yaml")).unwrap();
        assert!(content.contains("Kernel2"));
        assert!(content.contains("Kernel3"));
    }

    /// Test: Update ontology without concept name (should fail)
    #[test]
    fn test_update_ontology_no_concept() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        let kernel = Kernel::new(root, None, false);

        let changes = serde_json::json!({"metadata": {"description": "Test"}});
        let result = kernel.update_ontology(&changes);

        assert!(result.is_err());
        match result.unwrap_err() {
            CkpError::ParseError(msg) => {
                assert!(msg.contains("concept not set"));
            }
            _ => panic!("Expected ParseError"),
        }
    }

    // ----- load_instance() Tests (+3 tests) -----

    /// Test: Load instance by URN
    #[test]
    fn test_load_instance_success() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        // Create kernel with instance file
        let storage_dir = root.join("concepts/DataKernel/storage");
        fs::create_dir_all(&storage_dir).unwrap();

        let instance_data = serde_json::json!({
            "username": "alice",
            "email": "alice@example.com",
            "role": "admin"
        });

        fs::write(
            storage_dir.join("user-alice.inst"),
            serde_json::to_string_pretty(&instance_data).unwrap()
        ).unwrap();

        let kernel = Kernel::new(root.clone(), Some("DataKernel".to_string()), false);

        // Load instance
        let loaded = kernel.load_instance("ckp://DataKernel/storage/user-alice").unwrap();

        assert_eq!(loaded["username"], "alice");
        assert_eq!(loaded["email"], "alice@example.com");
        assert_eq!(loaded["role"], "admin");
    }

    /// Test: Load instance - non-existent file
    #[test]
    fn test_load_instance_not_found() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        let kernel = Kernel::new(root, Some("DataKernel".to_string()), false);

        let result = kernel.load_instance("ckp://DataKernel/storage/nonexistent");

        assert!(result.is_err());
        match result.unwrap_err() {
            CkpError::FileNotFound(msg) => {
                assert!(msg.contains("Failed to read instance"));
            }
            _ => panic!("Expected FileNotFound error"),
        }
    }

    /// Test: Load instance - various URN formats
    #[test]
    fn test_load_instance_urn_formats() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        let storage_dir = root.join("concepts/FormatKernel/storage");
        fs::create_dir_all(&storage_dir).unwrap();

        let data = serde_json::json!({"test": "data"});
        let data_str = serde_json::to_string_pretty(&data).unwrap();

        fs::write(storage_dir.join("test.inst"), &data_str).unwrap();

        let kernel = Kernel::new(root.clone(), Some("FormatKernel".to_string()), false);

        // Test various URN formats
        let result1 = kernel.load_instance("ckp://FormatKernel/storage/test");
        let result2 = kernel.load_instance("FormatKernel/storage/test");
        let result3 = kernel.load_instance("FormatKernel/test");

        assert!(result1.is_ok());
        assert!(result2.is_ok());
        assert!(result3.is_ok());
    }

    // ----- save_instance() Tests (+3 tests) -----

    /// Test: Save instance by URN
    #[test]
    fn test_save_instance_success() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        let kernel = Kernel::new(root.clone(), Some("SaveKernel".to_string()), false);

        let instance_data = serde_json::json!({
            "id": "user-123",
            "name": "Bob",
            "active": true
        });

        // Save instance
        kernel.save_instance("ckp://SaveKernel/storage/user-123", &instance_data).unwrap();

        // Verify file was created
        let instance_path = root.join("concepts/SaveKernel/storage/user-123.inst");
        assert!(instance_path.exists());

        // Verify content
        let content = fs::read_to_string(&instance_path).unwrap();
        let loaded: serde_json::Value = serde_json::from_str(&content).unwrap();

        assert_eq!(loaded["id"], "user-123");
        assert_eq!(loaded["name"], "Bob");
        assert_eq!(loaded["active"], true);
    }

    /// Test: Save instance - creates parent directories
    #[test]
    fn test_save_instance_creates_directories() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        let kernel = Kernel::new(root.clone(), Some("DirKernel".to_string()), false);

        let data = serde_json::json!({"nested": "data"});

        // Save to nested path (directories don't exist yet)
        kernel.save_instance("ckp://DirKernel/storage/nested/path/file", &data).unwrap();

        // Verify directories were created
        let file_path = root.join("concepts/DirKernel/storage/nested/path/file.inst");
        assert!(file_path.exists());

        // Verify content
        let content = fs::read_to_string(&file_path).unwrap();
        let loaded: serde_json::Value = serde_json::from_str(&content).unwrap();
        assert_eq!(loaded["nested"], "data");
    }

    /// Test: Save instance - overwrite existing
    #[test]
    fn test_save_instance_overwrite() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path().to_path_buf();

        let storage_dir = root.join("concepts/OverwriteKernel/storage");
        fs::create_dir_all(&storage_dir).unwrap();

        // Write initial instance
        let initial_data = serde_json::json!({"version": 1});
        fs::write(
            storage_dir.join("item.inst"),
            serde_json::to_string_pretty(&initial_data).unwrap()
        ).unwrap();

        let kernel = Kernel::new(root.clone(), Some("OverwriteKernel".to_string()), false);

        // Overwrite with new data
        let new_data = serde_json::json!({"version": 2, "updated": true});
        kernel.save_instance("ckp://OverwriteKernel/storage/item", &new_data).unwrap();

        // Verify new content
        let content = fs::read_to_string(storage_dir.join("item.inst")).unwrap();
        let loaded: serde_json::Value = serde_json::from_str(&content).unwrap();

        assert_eq!(loaded["version"], 2);
        assert_eq!(loaded["updated"], true);
    }
}
